{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-22T07:49:34.332840Z",
     "iopub.status.busy": "2024-11-22T07:49:34.332021Z",
     "iopub.status.idle": "2024-11-22T07:49:34.337201Z",
     "shell.execute_reply": "2024-11-22T07:49:34.336281Z",
     "shell.execute_reply.started": "2024-11-22T07:49:34.332805Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T07:29:26.307028Z",
     "iopub.status.busy": "2024-11-22T07:29:26.306735Z",
     "iopub.status.idle": "2024-11-22T07:29:26.322237Z",
     "shell.execute_reply": "2024-11-22T07:29:26.321479Z",
     "shell.execute_reply.started": "2024-11-22T07:29:26.306984Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 1 ->209\n",
    "# one -> 80\n",
    "# zero ->5733\n",
    "# 0 -> 632\n",
    "def replace_token_ids(target_encoding, replacement_map):\n",
    "    input_ids = target_encoding['input_ids']\n",
    "    for old_id, new_id in replacement_map.items():\n",
    "        input_ids[input_ids == old_id] = new_id  # Replace old ID with new ID\n",
    "    target_encoding['input_ids'] = input_ids\n",
    "    return target_encoding\n",
    "replacement_map = {5733: 632, 80: 209}\n",
    "label_map = {0: \"No\", 1: \"Yes\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T07:29:26.325161Z",
     "iopub.status.busy": "2024-11-22T07:29:26.324795Z",
     "iopub.status.idle": "2024-11-22T07:29:26.333685Z",
     "shell.execute_reply": "2024-11-22T07:29:26.332925Z",
     "shell.execute_reply.started": "2024-11-22T07:29:26.325136Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ToxicDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length=128):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        row = self.data.iloc[index]\n",
    "        input_text = row['text']\n",
    "        label = row['label']\n",
    "        explanation = row['explanation']\n",
    "        \n",
    "        # Prompt the model with instructions\n",
    "        input_prompt = f\"Is the below statement toxic (Yes/No)? Provide a brief explanation (20 words max). Statement: {input_text}\"\n",
    "        # target_text = f\"{label} Explanation: {explanation}\"\n",
    "        target_text = f\"{label_map[label]} Explanation: {explanation}\"\n",
    "        \n",
    "        input_encoding = self.tokenizer(\n",
    "            # input_text,\n",
    "            input_prompt,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        target_encoding = self.tokenizer(\n",
    "            target_text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        # target_encoding = replace_token_ids(target_encoding, replacement_map)\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_encoding[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": input_encoding[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": target_encoding[\"input_ids\"].squeeze(),\n",
    "            \"label_ids\": label,\n",
    "        }\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T07:29:26.335028Z",
     "iopub.status.busy": "2024-11-22T07:29:26.334698Z",
     "iopub.status.idle": "2024-11-22T07:29:34.150359Z",
     "shell.execute_reply": "2024-11-22T07:29:34.148922Z",
     "shell.execute_reply.started": "2024-11-22T07:29:26.334993Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b238f1806a2d466f86e9b3922e231e91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24704d4951bf4c0e8f06431bf03c2865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f05e746e4ab4d67a5a9b4e5e3153d17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b817fc153314c998761e4c1c9384b10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10e057150e3047c08703c40fefc2b75d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed592c7fdbd44864afec08486e97dd78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "138ec98d29da40bbbdb2f3d1de37aa3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Embedding(32101, 768)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\")\n",
    "# Add custom special tokens\n",
    "special_tokens = {\"bos_token\": \"<s>\", \"eos_token\": \"</s>\"}\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "# # Update the model to handle new tokens\n",
    "model.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T07:29:34.152002Z",
     "iopub.status.busy": "2024-11-22T07:29:34.151655Z",
     "iopub.status.idle": "2024-11-22T07:29:34.173133Z",
     "shell.execute_reply": "2024-11-22T07:29:34.172206Z",
     "shell.execute_reply.started": "2024-11-22T07:29:34.151965Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[32100,  2163,  1881,  3767,   257,    10,   100,    19,     3,   354,\n",
      "           157,    26,   115,     7,   107,  1191,     3,    40,     7,    23,\n",
      "            76,     9,   210,   157,  1824,  2741,    11, 25244,   581,  6578,\n",
      "             7,     5,    94,    31,     7,    73,  2666,    15,    11,     3,\n",
      "         15329,     5,     1,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:289: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "label = 1\n",
    "explanation =\"This is jkdbshja lsiuawkqear and prejudice against Asians.  It's untrue and inflammatory.\"\n",
    "target_text = f\"<s>{label_map[label]} Explanation: {explanation}</s>\"\n",
    "target_encoding = tokenizer(\n",
    "            target_text,\n",
    "            max_length=128,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "\n",
    "print(target_encoding)\n",
    "\n",
    "# Replace tokens\n",
    "# updated_target_encoding = replace_token_ids(target_encoding, replacement_map)\n",
    "\n",
    "# print(updated_target_encoding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T07:29:34.174642Z",
     "iopub.status.busy": "2024-11-22T07:29:34.174303Z",
     "iopub.status.idle": "2024-11-22T07:29:44.660867Z",
     "shell.execute_reply": "2024-11-22T07:29:44.659941Z",
     "shell.execute_reply.started": "2024-11-22T07:29:34.174604Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [2163, 1], 'attention_mask': [1, 1]}\n",
      "{'input_ids': [465, 1], 'attention_mask': [1, 1]}\n",
      "No\n",
      "Yes\n",
      "Yes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Yes 2163\n",
    "#No 465\n",
    "yes_token_id = tokenizer(\"Yes\")\n",
    "no_token_id = tokenizer(\"No\")\n",
    "print(yes_token_id)\n",
    "print(no_token_id)\n",
    "print(tokenizer.decode(4168))\n",
    "print(tokenizer.decode(19739))\n",
    "print(tokenizer.decode(2163))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T07:29:44.662488Z",
     "iopub.status.busy": "2024-11-22T07:29:44.662009Z",
     "iopub.status.idle": "2024-11-22T07:29:44.668268Z",
     "shell.execute_reply": "2024-11-22T07:29:44.667218Z",
     "shell.execute_reply.started": "2024-11-22T07:29:44.662461Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2163, 1], 'attention_mask': [1, 1]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T07:29:44.669584Z",
     "iopub.status.busy": "2024-11-22T07:29:44.669293Z",
     "iopub.status.idle": "2024-11-22T07:29:44.743854Z",
     "shell.execute_reply": "2024-11-22T07:29:44.743212Z",
     "shell.execute_reply.started": "2024-11-22T07:29:44.669560Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load dataset and dataloader\n",
    "dataset = ToxicDataset(\"/kaggle/input/geminiresponses1/response_train_filtered.csv\", tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T07:29:44.745220Z",
     "iopub.status.busy": "2024-11-22T07:29:44.744837Z",
     "iopub.status.idle": "2024-11-22T07:29:45.619691Z",
     "shell.execute_reply": "2024-11-22T07:29:45.618727Z",
     "shell.execute_reply.started": "2024-11-22T07:29:44.745181Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 8\n",
    "loss_fn = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T07:29:45.621184Z",
     "iopub.status.busy": "2024-11-22T07:29:45.620865Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        label_ids = batch[\"label_ids\"].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "\n",
    "        logits = outputs.logits\n",
    "        explanation_loss = outputs.loss\n",
    "        label_logits = logits[:, 1, :]  \n",
    "\n",
    "        label_token_ids = torch.tensor([465, 2163]).to(device)  \n",
    "\n",
    "\n",
    "        label_logits = label_logits[:, label_token_ids]  # Shape: [batch_size, 2]\n",
    "\n",
    "\n",
    "        label_loss = loss_fn(label_logits, label_ids)\n",
    "        \n",
    "        # Combine losses\n",
    "        loss = explanation_loss + label_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss / len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"fine_tuned_flan_t5_8\")\n",
    "tokenizer.save_pretrained(\"fine_tuned_flan_t5_8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the fine-tuned model\n",
    "# from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# tokenizer = T5Tokenizer.from_pretrained(\"fine_tuned_flan_t5\")\n",
    "# model = T5ForConditionalGeneration.from_pretrained(\"fine_tuned_flan_t5\")\n",
    "model.eval()\n",
    "test_df = pd.read_csv(\"/kaggle/input/geminiresponses1/response_train_filtered.csv\")\n",
    "\n",
    "for i in range(10):\n",
    "    input_text = test_df.iloc[i][\"text\"]\n",
    "    input_prompt = f\"Is the below statement toxic (Yes/No)? Provide a brief explanation (20 words max). Statement: {input_text}\"\n",
    "    input_encoding = tokenizer(input_prompt, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128)\n",
    "    input_ids = input_encoding[\"input_ids\"].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(input_ids).to(device)\n",
    "        decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(\"Input:\", input_text)\n",
    "    print(\"Output:\", decoded)\n",
    "\n",
    "    if \"Yes\" in decoded.split(\"Output:\")[-1]: \n",
    "            print(\"TOXIC\")\n",
    "            print()\n",
    "    elif \"No\" in decoded.split(\"Output:\")[-1]:  \n",
    "        print(\"NOT TOXIC\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "# def calculate_accuracy(model, tokenizer, data_df, max_length=128):\n",
    "#     model.eval()\n",
    "#     correct_count = 0\n",
    "#     total_count = 0\n",
    "#     all_preds = []\n",
    "#     all_labels = []\n",
    "\n",
    "#     for i in range(len(data_df)):\n",
    "#         input_text = data_df.iloc[i][\"text\"]\n",
    "#         true_label = data_df.iloc[i][\"label\"]  \n",
    "\n",
    "#         # Create the input prompt\n",
    "#         input_prompt = f\"Is the below statement toxic (Yes/No)? Provide a brief explanation (20 words max). Statement: {input_text}\"\n",
    "#         input_encoding = tokenizer(\n",
    "#             input_prompt,\n",
    "#             return_tensors=\"pt\",\n",
    "#             truncation=True,\n",
    "#             padding=\"max_length\",\n",
    "#             max_length=max_length\n",
    "#         )\n",
    "#         input_ids = input_encoding[\"input_ids\"].to(device)\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model.generate(input_ids, max_length=max_length)\n",
    "#             decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "      \n",
    "#         predicted_label = -1  \n",
    "#         if \"Yes\" in decoded.split(\"Output:\")[-1]:  \n",
    "#             predicted_label = 1\n",
    "#         elif \"No\" in decoded.split(\"Output:\")[-1]:  \n",
    "#             predicted_label = 0\n",
    "\n",
    "#         # if predicted_label == true_label:\n",
    "#         #     correct_count += 1\n",
    "#         all_preds.append(predicted_label)\n",
    "#         all_labels.append(true_label)\n",
    "\n",
    "#     # Calculate accuracy\n",
    "#     # accuracy = correct_count / len(data_df)\n",
    "#     accuracy = accuracy_score(all_labels, all_preds)\n",
    "#     f1 = f1_score(all_labels, all_preds)\n",
    "#     auc = roc_auc_score(all_labels, all_preds)\n",
    "#     return accuracy, f1, auc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Calculate test accuracy\n",
    "# test_accuracy, test_f1, test_auc = calculate_accuracy(\n",
    "#     model, tokenizer, pd.read_csv(\"/kaggle/input/geminiresponses1/response_test_filtered.csv\")\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# print(f\"Test Accuracy: {test_accuracy:.2%}\")\n",
    "# print(f\"Test F1: {test_f1 : .2}\")\n",
    "# print(f\"Test AUC: {test_auc : .2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"fine_tuned_flan_t5_8\")\n",
    "tokenizer.save_pretrained(\"fine_tuned_flan_t5_8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T07:49:46.513361Z",
     "iopub.status.busy": "2024-11-22T07:49:46.513038Z",
     "iopub.status.idle": "2024-11-22T07:49:46.521028Z",
     "shell.execute_reply": "2024-11-22T07:49:46.520208Z",
     "shell.execute_reply.started": "2024-11-22T07:49:46.513334Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy_1(model, tokenizer, data_df, max_length=128):\n",
    "    model.eval()\n",
    "    correct_count = 0\n",
    "    total_count = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for i in range(len(data_df)):\n",
    "        input_text = data_df.iloc[i][\"text\"]\n",
    "        true_label = data_df.iloc[i][\"toxicity_human\"]  \n",
    "\n",
    "        # Create the input prompt\n",
    "        input_prompt = f\"Is the below statement toxic (Yes/No)? Provide a brief explanation (20 words max). Statement: {input_text}\"\n",
    "        input_encoding = tokenizer(\n",
    "            input_prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length\n",
    "        )\n",
    "        input_ids = input_encoding[\"input_ids\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(input_ids, max_length=max_length)\n",
    "            decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "      \n",
    "        predicted_label = -1  \n",
    "        if \"Yes\" in decoded.split(\"Output:\")[-1]:  \n",
    "            predicted_label = 1\n",
    "        elif \"No\" in decoded.split(\"Output:\")[-1]:  \n",
    "            predicted_label = 0\n",
    "\n",
    "        # if predicted_label == true_label:\n",
    "        #     correct_count += 1\n",
    "        all_preds.append(predicted_label)\n",
    "        all_labels.append(true_label)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    # accuracy = correct_count / len(data_df)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    auc = roc_auc_score(all_labels, all_preds)\n",
    "    return accuracy, f1, auc, all_labels, all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Calculate test accuracy\n",
    "test_accuracy, test_f1, test_auc = calculate_accuracy_1(\n",
    "    model, tokenizer, pd.read_csv(\"/kaggle/input/toxichuman/test_response.csv\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f\"Test Accuracy: {test_accuracy:.2%}\")\n",
    "print(f\"Test F1: {test_f1 : .2}\")\n",
    "print(f\"Test AUC: {test_auc : .2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T07:50:06.103105Z",
     "iopub.status.busy": "2024-11-22T07:50:06.102289Z",
     "iopub.status.idle": "2024-11-22T07:50:07.365801Z",
     "shell.execute_reply": "2024-11-22T07:50:07.364833Z",
     "shell.execute_reply.started": "2024-11-22T07:50:06.103070Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "# from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "# import torch\n",
    "\n",
    "\n",
    "# # Path to the fine-tuned model directory\n",
    "# model_dir = \"/kaggle/input/finalmodel/pytorch/default/1\"\n",
    "\n",
    "# # Load the tokenizer\n",
    "# tokenizer = T5Tokenizer.from_pretrained(model_dir)\n",
    "\n",
    "# # Load the model\n",
    "# model = T5ForConditionalGeneration.from_pretrained(model_dir)\n",
    "\n",
    "# # Move the model to the device (e.g., GPU if available)\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model.to(device)\n",
    "\n",
    "# print(\"Model and tokenizer successfully loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T07:50:19.923774Z",
     "iopub.status.busy": "2024-11-22T07:50:19.923115Z",
     "iopub.status.idle": "2024-11-22T07:57:55.900719Z",
     "shell.execute_reply": "2024-11-22T07:57:55.899982Z",
     "shell.execute_reply.started": "2024-11-22T07:50:19.923741Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Calculate test accuracy\n",
    "test_accuracy, test_f1, test_auc, true_labels, pred_labels = calculate_accuracy_1(\n",
    "    model, tokenizer, pd.read_csv(\"/kaggle/input/toxichuman/test_response.csv\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T07:59:48.618975Z",
     "iopub.status.busy": "2024-11-22T07:59:48.618622Z",
     "iopub.status.idle": "2024-11-22T07:59:48.624117Z",
     "shell.execute_reply": "2024-11-22T07:59:48.623253Z",
     "shell.execute_reply.started": "2024-11-22T07:59:48.618948Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 79.91%\n",
      "Test F1:  0.77\n",
      "Test AUC:  0.8\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test Accuracy: {test_accuracy:.2%}\")\n",
    "print(f\"Test F1: {test_f1 : .2}\")\n",
    "print(f\"Test AUC: {test_auc : .2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T08:00:05.705107Z",
     "iopub.status.busy": "2024-11-22T08:00:05.704462Z",
     "iopub.status.idle": "2024-11-22T08:00:05.712787Z",
     "shell.execute_reply": "2024-11-22T08:00:05.711850Z",
     "shell.execute_reply.started": "2024-11-22T08:00:05.705072Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive Percentage: 33.22%\n",
      "True Negative Percentage: 46.69%\n",
      "False Positive Percentage: 12.69%\n",
      "False Negative Percentage: 7.40%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_percentages(true_labels, pred_labels):\n",
    "    # Convert to numpy arrays for easy indexing\n",
    "    true_labels = np.array(true_labels)\n",
    "    pred_labels = np.array(pred_labels)\n",
    "\n",
    "    # Calculate TP, TN, FP, FN\n",
    "    TP = np.sum((true_labels == 1) & (pred_labels == 1))\n",
    "    TN = np.sum((true_labels == 0) & (pred_labels == 0))\n",
    "    FP = np.sum((true_labels == 0) & (pred_labels == 1))\n",
    "    FN = np.sum((true_labels == 1) & (pred_labels == 0))\n",
    "\n",
    "    # Total samples\n",
    "    total_samples = len(true_labels)\n",
    "\n",
    "    # Calculate percentages\n",
    "    TP_percentage = (TP / total_samples) * 100\n",
    "    TN_percentage = (TN / total_samples) * 100\n",
    "    FP_percentage = (FP / total_samples) * 100\n",
    "    FN_percentage = (FN / total_samples) * 100\n",
    "\n",
    "    return TP_percentage, TN_percentage, FP_percentage, FN_percentage\n",
    "\n",
    "TP_percentage, TN_percentage, FP_percentage, FN_percentage = calculate_percentages(true_labels, pred_labels)\n",
    "\n",
    "print(f\"True Positive Percentage: {TP_percentage:.2f}%\")\n",
    "print(f\"True Negative Percentage: {TN_percentage:.2f}%\")\n",
    "print(f\"False Positive Percentage: {FP_percentage:.2f}%\")\n",
    "print(f\"False Negative Percentage: {FN_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T08:02:12.412583Z",
     "iopub.status.busy": "2024-11-22T08:02:12.411962Z",
     "iopub.status.idle": "2024-11-22T08:02:12.419661Z",
     "shell.execute_reply": "2024-11-22T08:02:12.418777Z",
     "shell.execute_reply.started": "2024-11-22T08:02:12.412536Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy_2(model, tokenizer, data_df, max_length=128):\n",
    "    model.eval()\n",
    "    correct_count = 0\n",
    "    total_count = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for i in tqdm(range(len(data_df))):\n",
    "        input_text = data_df.iloc[i][\"text\"]\n",
    "        true_label = data_df.iloc[i][\"toxic\"]  \n",
    "\n",
    "        # Create the input prompt\n",
    "        input_prompt = f\"Is the below statement toxic (Yes/No)? Provide a brief explanation (20 words max). Statement: {input_text}\"\n",
    "        input_encoding = tokenizer(\n",
    "            input_prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length\n",
    "        )\n",
    "        input_ids = input_encoding[\"input_ids\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(input_ids, max_length=max_length)\n",
    "            decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "      \n",
    "        predicted_label = -1  \n",
    "        if \"Yes\" in decoded.split(\"Output:\")[-1]:  \n",
    "            predicted_label = 1\n",
    "        elif \"No\" in decoded.split(\"Output:\")[-1]:  \n",
    "            predicted_label = 0\n",
    "\n",
    "        # if predicted_label == true_label:\n",
    "        #     correct_count += 1\n",
    "        all_preds.append(predicted_label)\n",
    "        all_labels.append(true_label)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    # accuracy = correct_count / len(data_df)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    auc = roc_auc_score(all_labels, all_preds)\n",
    "    return accuracy, f1, auc, all_labels, all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T08:34:39.766340Z",
     "iopub.status.busy": "2024-11-22T08:34:39.765511Z",
     "iopub.status.idle": "2024-11-22T09:10:08.852275Z",
     "shell.execute_reply": "2024-11-22T09:10:08.851297Z",
     "shell.execute_reply.started": "2024-11-22T08:34:39.766307Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [35:29<00:00,  1.88it/s]\n"
     ]
    }
   ],
   "source": [
    "# Calculate test accuracy\n",
    "test_accuracy, test_f1, test_auc, true_labels, pred_labels = calculate_accuracy_2(\n",
    "    model, tokenizer, pd.read_csv(\"/kaggle/input/dataset1/toxicity_parsed_dataset.csv\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T09:10:08.853981Z",
     "iopub.status.busy": "2024-11-22T09:10:08.853686Z",
     "iopub.status.idle": "2024-11-22T09:10:08.858871Z",
     "shell.execute_reply": "2024-11-22T09:10:08.857860Z",
     "shell.execute_reply.started": "2024-11-22T09:10:08.853955Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 86.83%\n",
      "Test F1:  0.87\n",
      "Test AUC:  0.87\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test Accuracy: {test_accuracy:.2%}\")\n",
    "print(f\"Test F1: {test_f1 : .2}\")\n",
    "print(f\"Test AUC: {test_auc : .2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T09:10:08.860182Z",
     "iopub.status.busy": "2024-11-22T09:10:08.859897Z",
     "iopub.status.idle": "2024-11-22T09:10:08.871959Z",
     "shell.execute_reply": "2024-11-22T09:10:08.871147Z",
     "shell.execute_reply.started": "2024-11-22T09:10:08.860159Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive Percentage: 44.98%\n",
      "True Negative Percentage: 41.85%\n",
      "False Positive Percentage: 8.15%\n",
      "False Negative Percentage: 5.03%\n"
     ]
    }
   ],
   "source": [
    "TP_percentage, TN_percentage, FP_percentage, FN_percentage = calculate_percentages(true_labels, pred_labels)\n",
    "\n",
    "print(f\"True Positive Percentage: {TP_percentage:.2f}%\")\n",
    "print(f\"True Negative Percentage: {TN_percentage:.2f}%\")\n",
    "print(f\"False Positive Percentage: {FP_percentage:.2f}%\")\n",
    "print(f\"False Negative Percentage: {FN_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T08:34:29.793587Z",
     "iopub.status.busy": "2024-11-22T08:34:29.793096Z",
     "iopub.status.idle": "2024-11-22T08:34:29.797251Z",
     "shell.execute_reply": "2024-11-22T08:34:29.796456Z",
     "shell.execute_reply.started": "2024-11-22T08:34:29.793536Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# white slavery (18th-19th century. mostly white women. still going on in some places) - people of color in general\n",
    "# obama is a muslim,\n",
    "# i was walking down the street the other day and i saw a group of about 5 latino men walking towards me.\n",
    "# i dont want to live in a world where i have to learn mandarin to get along\n",
    "# if we don't learn about the past, we wont learn about the present and future2I\n",
    "# not all of them eat cats and dogs, some of them don't even know what that is   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T09:14:33.896933Z",
     "iopub.status.busy": "2024-11-22T09:14:33.896175Z",
     "iopub.status.idle": "2024-11-22T09:14:34.590809Z",
     "shell.execute_reply": "2024-11-22T09:14:34.589939Z",
     "shell.execute_reply.started": "2024-11-22T09:14:33.896902Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if we don't learn about the past, we wont learn about the present and future2I\n",
      "Yes Explanation: This statement is a harmful and untrue generalization that promotes a harmful and untrue stereotype about people. It's prejudiced and harmful.\n"
     ]
    }
   ],
   "source": [
    "input_text = \"if we don't learn about the past, we wont learn about the present and future2I\"\n",
    "input_prompt = f\"Is the below statement toxic (Yes/No)? Provide a brief explanation (20 words max). Statement: {input_text}\"\n",
    "input_encoding = tokenizer(\n",
    "    input_prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    max_length=128\n",
    ")\n",
    "input_ids = input_encoding[\"input_ids\"].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(input_ids, max_length=128)\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(input_text)\n",
    "    print(decoded)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6135593,
     "sourceId": 9972676,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6140384,
     "sourceId": 9979196,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6141219,
     "sourceId": 9980325,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 171384,
     "modelInstanceId": 148874,
     "sourceId": 174865,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
